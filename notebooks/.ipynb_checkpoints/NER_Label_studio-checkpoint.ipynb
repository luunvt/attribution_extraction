{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FiX DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "f = open(\"../data/label_ner_1_1015.json\")\n",
    "\n",
    "def adding_space(s):\n",
    "    single_description = s.lower().strip()\n",
    "    new_description = []\n",
    "    last_special = -1\n",
    "    for idx, letter in enumerate(single_description):\n",
    "        if not (('a' <= letter and letter <= 'z') or ('0' <= letter and letter <= '9') or letter == ' '):\n",
    "            pretext = single_description[last_special + 1:idx].strip()\n",
    "            if pretext != '' and pretext != ' ':\n",
    "                new_description.append(pretext)\n",
    "            new_description.append(letter.strip())\n",
    "            last_special = idx\n",
    "        if idx == len(single_description) - 1:\n",
    "            new_description.append(single_description[last_special + 1:idx + 1].strip())\n",
    "\n",
    "    new_description = ' '.join(new_description).lower().strip()\n",
    "    return new_description\n",
    "\n",
    "list_data = json.load(f)\n",
    "list_sentence = []\n",
    "list_locs = []\n",
    "list_words = []\n",
    "list_id = []\n",
    "for i_list_data, data in enumerate(list_data):\n",
    "    original_description = data['data']['text']\n",
    "    original_annotations = data['annotations']\n",
    "\n",
    "    new_description = adding_space(original_description)\n",
    "\n",
    "    locs = []\n",
    "    words = []\n",
    "    set_new_locations = set()\n",
    "    for annotation in original_annotations:\n",
    "        results = annotation['result']\n",
    "        for ia, result in enumerate(results):\n",
    "            label = result['value']['labels'][0]\n",
    "            org_start = result['value']['start']\n",
    "            org_end = result['value']['end']\n",
    "            org_word = result['value']['text'].lower().strip()\n",
    "            new_word = adding_space(org_word)\n",
    "            word_len = len(new_word)\n",
    "            new_start = None\n",
    "            new_end = None\n",
    "            for i in range(org_start, len(new_description)):\n",
    "                if new_description[i:i+word_len] == new_word and (i, i+word_len) not in set_new_locations:\n",
    "                    new_start = i\n",
    "                    new_end = i + word_len\n",
    "                    set_new_locations.add((new_start, new_end))\n",
    "                    list_data[i_list_data]['annotations'][0]['result'][ia]['value']['text'] = new_word\n",
    "                    list_data[i_list_data]['annotations'][0]['result'][ia]['value']['start'] = new_start\n",
    "                    list_data[i_list_data]['annotations'][0]['result'][ia]['value']['end'] = new_end\n",
    "                    break\n",
    "    list_data[i_list_data]['data']['text'] = new_description\n",
    "    if len(set_new_locations) != len(annotation['result']):\n",
    "        print(i_list_data)\n",
    "        # print(original_description)\n",
    "        # print(new_description)\n",
    "        # display(original_annotations)\n",
    "        # print(set_new_locations)\n",
    "        # display(annotation['result'])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/label_ner_1_1015_fixed.json\", \"w\") as outfile:\n",
    "    json.dump(list_data, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA:  True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "print(\"CUDA: \", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/tanluuuuuuu/miniconda3/envs/one_for_all/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# https://huggingface.co/docs/transformers/tasks/token_classification\n",
    "# model_checkpoint = \"distilbert-base-uncased\"\n",
    "# model_checkpoint = \"./bert-finetuned-ner\"\n",
    "# model_checkpoint = \"bert-base-cased\"\n",
    "# model_checkpoint = \"dslim/bert-base-NER\"\n",
    "# model_checkpoint = \"roberta-base\"\n",
    "model_checkpoint = \"microsoft/deberta-v3-large\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_LABELS = ['AGE_GROUP', 'GENDER', 'FEATURE', 'TARGET_USAGE', 'MAIN_PRODUCT', 'COMPONENT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAT\n",
      "NMAT\n",
      "DIMENSION\n",
      "COMPONENT\n",
      "MAIN_PRODUCT\n",
      "WEIGHT\n",
      "TARGET_USER\n",
      "PROPERTY\n",
      "COLOR\n",
      "SHAPE\n",
      "['O', 'B-MAT', 'I-MAT', 'B-NMAT', 'I-NMAT', 'B-DIMENSION', 'I-DIMENSION', 'B-WEIGHT', 'I-WEIGHT', 'B-TARGET_USER', 'I-TARGET_USER', 'B-PROPERTY', 'I-PROPERTY', 'B-COLOR', 'I-COLOR', 'B-SHAPE', 'I-SHAPE']\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "ner_tags_doccano = [\n",
    "    {\n",
    "        \"id\": 6,\n",
    "        \"text\": \"MAT\",\n",
    "\n",
    "    },\n",
    "    {\n",
    "        \"id\": 7,\n",
    "        \"text\": \"NMAT\",\n",
    "\n",
    "    },\n",
    "    {\n",
    "        \"id\": 8,\n",
    "        \"text\": \"DIMENSION\",\n",
    "\n",
    "    },\n",
    "    {\n",
    "        \"id\": 11,\n",
    "        \"text\": \"COMPONENT\",\n",
    "\n",
    "    },\n",
    "    {\n",
    "        \"id\": 13,\n",
    "        \"text\": \"MAIN_PRODUCT\",\n",
    "\n",
    "    },\n",
    "    {\n",
    "        \"id\": 14,\n",
    "        \"text\": \"WEIGHT\",\n",
    "\n",
    "    },\n",
    "    {\n",
    "        \"id\": 15,\n",
    "        \"text\": \"TARGET_USER\",\n",
    "\n",
    "    },\n",
    "    {\n",
    "        \"id\": 17,\n",
    "        \"text\": \"PROPERTY\",\n",
    "\n",
    "    },\n",
    "    {\n",
    "        \"id\": 18,\n",
    "        \"text\": \"COLOR\",\n",
    "\n",
    "    },\n",
    "    {\n",
    "        \"id\": 19,\n",
    "        \"text\": \"SHAPE\",\n",
    "\n",
    "    }\n",
    "]\n",
    "\n",
    "processed_ner_tags = ['O']\n",
    "for tag in ner_tags_doccano:\n",
    "    if tag['text'] not in REMOVE_LABELS:\n",
    "        processed_ner_tags.append(f\"B-{tag['text']}\")\n",
    "        processed_ner_tags.append(f\"I-{tag['text']}\")\n",
    "        print(tag['text'])\n",
    "    else:\n",
    "        print(tag['text'])\n",
    "\n",
    "print(processed_ner_tags)\n",
    "print(len(processed_ner_tags))\n",
    "\n",
    "ner_tags_2_number = {t: i for (i, t) in enumerate(processed_ner_tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>locs</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>üëç „Äê folding &amp; portable rebounder „Äë the jump tr...</td>\n",
       "      <td>[(4, 11, PROPERTY), (14, 32, PROPERTY), (222, ...</td>\n",
       "      <td>[folding, portable rebounder, portable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uv printing</td>\n",
       "      <td>[(0, 2, MAT)]</td>\n",
       "      <td>[uv]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>premium eco friendly material with no odor : o...</td>\n",
       "      <td>[(123, 133, NMAT), (50, 66, PROPERTY), (35, 42...</td>\n",
       "      <td>[phthalates, non - allergenic, no odor, non - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boxing speed ball : pu material speed ball , c...</td>\n",
       "      <td>[(20, 22, MAT), (105, 118, PROPERTY), (79, 89,...</td>\n",
       "      <td>[pu, without sound, 90 degrees, rebound]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weighs 100 lbs</td>\n",
       "      <td>[(7, 14, WEIGHT)]</td>\n",
       "      <td>[100 lbs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2056</th>\n",
       "      <td>uv protection : light filtering soft sail shad...</td>\n",
       "      <td>[(16, 31, PROPERTY), (32, 54, PROPERTY), (66, ...</td>\n",
       "      <td>[light filtering, soft sail shade canopy, soft...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2057</th>\n",
       "      <td>powered up : exclusive removable powerport fea...</td>\n",
       "      <td>[(23, 32, PROPERTY), (52, 72, PROPERTY), (77, ...</td>\n",
       "      <td>[removable, 3 electrical outlets, 3 usb ports]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2058</th>\n",
       "      <td>steel anchoring plates : powder coated steel a...</td>\n",
       "      <td>[(0, 22, PROPERTY), (25, 61, PROPERTY), (72, 9...</td>\n",
       "      <td>[steel anchoring plates, powder coated steel a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2059</th>\n",
       "      <td>easy assembly : supported by step - by - step ...</td>\n",
       "      <td>[(58, 66, PROPERTY)]</td>\n",
       "      <td>[bilt app]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2060</th>\n",
       "      <td>„Äê fitness and fat burning „Äë the training tubes...</td>\n",
       "      <td>[(147, 157, PROPERTY), (175, 186, PROPERTY)]</td>\n",
       "      <td>[resistance, color coded]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2061 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  \\\n",
       "0     üëç „Äê folding & portable rebounder „Äë the jump tr...   \n",
       "1                                           uv printing   \n",
       "2     premium eco friendly material with no odor : o...   \n",
       "3     boxing speed ball : pu material speed ball , c...   \n",
       "4                                        weighs 100 lbs   \n",
       "...                                                 ...   \n",
       "2056  uv protection : light filtering soft sail shad...   \n",
       "2057  powered up : exclusive removable powerport fea...   \n",
       "2058  steel anchoring plates : powder coated steel a...   \n",
       "2059  easy assembly : supported by step - by - step ...   \n",
       "2060  „Äê fitness and fat burning „Äë the training tubes...   \n",
       "\n",
       "                                                   locs  \\\n",
       "0     [(4, 11, PROPERTY), (14, 32, PROPERTY), (222, ...   \n",
       "1                                         [(0, 2, MAT)]   \n",
       "2     [(123, 133, NMAT), (50, 66, PROPERTY), (35, 42...   \n",
       "3     [(20, 22, MAT), (105, 118, PROPERTY), (79, 89,...   \n",
       "4                                     [(7, 14, WEIGHT)]   \n",
       "...                                                 ...   \n",
       "2056  [(16, 31, PROPERTY), (32, 54, PROPERTY), (66, ...   \n",
       "2057  [(23, 32, PROPERTY), (52, 72, PROPERTY), (77, ...   \n",
       "2058  [(0, 22, PROPERTY), (25, 61, PROPERTY), (72, 9...   \n",
       "2059                               [(58, 66, PROPERTY)]   \n",
       "2060       [(147, 157, PROPERTY), (175, 186, PROPERTY)]   \n",
       "\n",
       "                                                  words  \n",
       "0               [folding, portable rebounder, portable]  \n",
       "1                                                  [uv]  \n",
       "2     [phthalates, non - allergenic, no odor, non - ...  \n",
       "3              [pu, without sound, 90 degrees, rebound]  \n",
       "4                                             [100 lbs]  \n",
       "...                                                 ...  \n",
       "2056  [light filtering, soft sail shade canopy, soft...  \n",
       "2057     [removable, 3 electrical outlets, 3 usb ports]  \n",
       "2058  [steel anchoring plates, powder coated steel a...  \n",
       "2059                                         [bilt app]  \n",
       "2060                          [resistance, color coded]  \n",
       "\n",
       "[2061 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# file_path = '../data/label_studio_1017_shorter.json'\n",
    "label_counter = Counter()\n",
    "\n",
    "def read_data_json(list_file_path):\n",
    "    list_data = []\n",
    "    for file_path in list_file_path:\n",
    "        f = open(file_path)\n",
    "\n",
    "        list_data += json.load(f)\n",
    "\n",
    "    list_sentence = []\n",
    "    list_locs = []\n",
    "    list_words = []\n",
    "    for i, data in enumerate(list_data):\n",
    "        text = data['data']['text']\n",
    "        annotations = data['annotations']\n",
    "        locs = []\n",
    "        words = []\n",
    "        for annotation in annotations:\n",
    "            results = annotation['result']\n",
    "            for result in results:\n",
    "                start = result['value']['start']\n",
    "                end = result['value']['end']\n",
    "                label = result['value']['labels'][0]\n",
    "\n",
    "                if label not in REMOVE_LABELS:\n",
    "                    label_counter[label] += 1\n",
    "                    locs.append((str(start), str(end), str(label)))\n",
    "                    words.append(text[start:end])\n",
    "\n",
    "        list_sentence.append(text)\n",
    "        list_locs.append(locs)\n",
    "        list_words.append(words)\n",
    "\n",
    "    raw_dataset = pd.DataFrame()\n",
    "    raw_dataset['sentence'] = pd.Series(list_sentence)\n",
    "    raw_dataset['locs'] = pd.Series(list_locs)\n",
    "    raw_dataset['words'] = pd.Series(list_words)\n",
    "\n",
    "    raw_dataset.to_excel(\"./raw_dataset.xlsx\", index=False)\n",
    "    return raw_dataset\n",
    "\n",
    "def read_data_colnn(file_path):\n",
    "    file_colnn = open(file_path, 'r')\n",
    "    lines = file_colnn.readlines()\n",
    "\n",
    "    list_words = []\n",
    "    list_labels = []\n",
    "\n",
    "    word_pieces = []\n",
    "    label_pieces = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        line_split = line.split(\" \")\n",
    "\n",
    "        if line_split[0] == \"-DOCSTART-\":\n",
    "            continue\n",
    "        elif line_split[0] == \"\":\n",
    "            list_words.append(word_pieces)\n",
    "            list_labels.append(label_pieces)\n",
    "\n",
    "            word_pieces = []\n",
    "            label_pieces = []\n",
    "            continue\n",
    "\n",
    "        word = line_split[0]\n",
    "        label = line_split[3]\n",
    "\n",
    "        if label in REMOVE_LABELS:\n",
    "            label = \"O\"\n",
    "            \n",
    "        label = ner_tags_2_number[label]\n",
    "\n",
    "        word_pieces.append(word)\n",
    "        label_pieces.append(label)\n",
    "\n",
    "    raw_dataset = pd.DataFrame()\n",
    "    raw_dataset['tokens'] = pd.Series(list_words)\n",
    "    raw_dataset['ner_tags'] = pd.Series(list_labels)\n",
    "    return raw_dataset\n",
    "\n",
    "raw_dataset = read_data_json([\"../data/label_ner_1_1015_fixed.json\", \n",
    "                               \"../data/label_ner_2_1044.json\"])\n",
    "display(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'PROPERTY': 2973,\n",
       "         'MAT': 647,\n",
       "         'NMAT': 170,\n",
       "         'WEIGHT': 202,\n",
       "         'TARGET_USER': 768,\n",
       "         'DIMENSION': 521,\n",
       "         'COLOR': 112,\n",
       "         'SHAPE': 28})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# creating the dataset\n",
    "courses = list(label_counter.keys())\n",
    "values = list(label_counter.values())\n",
    "\n",
    "fig = plt.figure(figsize = (15, 5))\n",
    "\n",
    "# creating the bar plotlabel_studio_1017_shorter.json\n",
    "plt.bar(courses, values, color ='maroon',\n",
    "\t\twidth = 0.4)\n",
    "\n",
    "plt.xlabel(\"Label name\")\n",
    "plt.ylabel(\"Number of instance\")\n",
    "plt.title(\"Label statistic\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence    üëç „Äê folding & portable rebounder „Äë the jump tr...\n",
       "locs        [(4, 11, PROPERTY), (14, 32, PROPERTY), (222, ...\n",
       "words                 [folding, portable rebounder, portable]\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'locs', 'words'],\n",
       "        num_rows: 2061\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['sentence', 'locs', 'words'],\n",
       "        num_rows: 2061\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'locs', 'words'],\n",
       "        num_rows: 2061\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict, ClassLabel, Features, Value, Sequence\n",
    "\n",
    "train_dataset = Dataset.from_pandas(raw_dataset)\n",
    "val_dataset = Dataset.from_pandas(raw_dataset)\n",
    "test_dataset = Dataset.from_pandas(raw_dataset)\n",
    "\n",
    "raw_datasets = DatasetDict(\n",
    "    {\"train\": train_dataset, \"val\": val_dataset, \"test\": test_dataset})\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_ner_tags(words, labels):\n",
    "    line1 = \"\"\n",
    "    line2 = \"\"\n",
    "    for word, label in zip(words, labels):\n",
    "        # print(word, label)\n",
    "        label = str(label)\n",
    "        max_length = max(len(word), len(label))\n",
    "        line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "        line2 += label + \" \" * (max_length - len(label) + 1)\n",
    "    print(line1)\n",
    "    print(line2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "def assign_ner_tags_bert(example):\n",
    "    # print(example['sentence'])\n",
    "    token_input = tokenizer(example['sentence'])\n",
    "    example['tokens'] = tokenizer.convert_ids_to_tokens(\n",
    "        token_input['input_ids'])\n",
    "\n",
    "    # print(example['sentence'])\n",
    "    # print(example['tokens'])\n",
    "    # print(\"len: \", len(example['tokens']))\n",
    "\n",
    "    ner_tags = [0 for token in example['tokens']]\n",
    "    if str(type(example['locs'])) == \"<class 'list'>\":\n",
    "        locs = example['locs']\n",
    "    else:\n",
    "        locs = ast.literal_eval(example['locs'])\n",
    "\n",
    "    locs = [(int(loc[0]), int(loc[1]), loc[2]) for loc in locs]\n",
    "    locs = sorted(locs)\n",
    "    # print(locs)\n",
    "    bg_id = 1\n",
    "    pre_loc = 0\n",
    "    text = example['sentence']\n",
    "    for loc in locs:\n",
    "        loc0 = int(loc[0])\n",
    "        loc1 = int(loc[1])\n",
    "\n",
    "        pre_text = text[pre_loc:loc0]\n",
    "        token_input = tokenizer(pre_text)\n",
    "        pre_token = tokenizer.convert_ids_to_tokens(\n",
    "            token_input['input_ids'])\n",
    "\n",
    "        # print(pre_text)\n",
    "        # print(pre_token)\n",
    "        # print(\"len: \", len(pre_token))\n",
    "\n",
    "        bg_id = bg_id + len(pre_token) - 2\n",
    "        pre_loc = loc1\n",
    "\n",
    "        # print(\"bg_id: \", bg_id)\n",
    "\n",
    "        word = example['sentence'][loc0: loc1]\n",
    "        token_input = tokenizer(word)\n",
    "        word_token = tokenizer.convert_ids_to_tokens(token_input['input_ids'])\n",
    "\n",
    "        # print(\"Word: \", word)\n",
    "        # print(word_token)\n",
    "        # print(\"len: \", len(word_token))\n",
    "\n",
    "        label_number = ner_tags_2_number[f\"B-{loc[2]}\"]\n",
    "        ner_tags[bg_id] = label_number\n",
    "        bg_id += 1\n",
    "        for idx in range(bg_id, bg_id + len(word_token) - 3):\n",
    "            ner_tags[idx] = label_number + 1\n",
    "        bg_id = bg_id + len(word_token) - 3\n",
    "\n",
    "        # visualize_ner_tags(example['tokens'], ner_tags)\n",
    "\n",
    "    ner_tags[0] = -100\n",
    "    ner_tags[-1] = -100\n",
    "    return ner_tags\n",
    "\n",
    "def assign_ner_tags_deberta(example):\n",
    "    # print(example['sentence'])\n",
    "    token_input = tokenizer(example['sentence'])\n",
    "    example['tokens'] = tokenizer.convert_ids_to_tokens(\n",
    "        token_input['input_ids'])\n",
    "\n",
    "    # print(example['sentence'])\n",
    "    # print(example['tokens'])\n",
    "    # print(\"len: \", len(example['tokens']))\n",
    "\n",
    "    ner_tags = [0 for token in example['tokens']]\n",
    "    if str(type(example['locs'])) == \"<class 'list'>\":\n",
    "        locs = example['locs']\n",
    "    else:\n",
    "        locs = ast.literal_eval(example['locs'])\n",
    "\n",
    "    locs = [(int(loc[0]), int(loc[1]), loc[2]) for loc in locs]\n",
    "    locs = sorted(locs)\n",
    "    # print(locs)\n",
    "    bg_id = 1\n",
    "    pre_loc = 0\n",
    "    text = example['sentence']\n",
    "    for loc in locs:\n",
    "        loc0 = int(loc[0])\n",
    "        loc1 = int(loc[1])\n",
    "\n",
    "        pre_text = text[pre_loc:loc0]\n",
    "        token_input = tokenizer(pre_text)\n",
    "        pre_token = tokenizer.convert_ids_to_tokens(\n",
    "            token_input['input_ids'])\n",
    "\n",
    "        print(pre_text)\n",
    "        print(pre_token)\n",
    "        print(\"len: \", len(pre_token))\n",
    "\n",
    "        bg_id = bg_id + len(pre_token) - 2\n",
    "        pre_loc = loc1\n",
    "\n",
    "        # print(\"bg_id: \", bg_id)\n",
    "\n",
    "        word = example['sentence'][loc0: loc1]\n",
    "        token_input = tokenizer(word)\n",
    "        word_token = tokenizer.convert_ids_to_tokens(token_input['input_ids'])\n",
    "\n",
    "        print(\"Word: \", word)\n",
    "        print(word_token)\n",
    "        print(\"len: \", len(word_token))\n",
    "\n",
    "        label_number = ner_tags_2_number[f\"B-{loc[2]}\"]\n",
    "        ner_tags[bg_id] = label_number\n",
    "        bg_id += 1\n",
    "        for idx in range(bg_id, bg_id + len(word_token) - 3):\n",
    "            ner_tags[idx] = label_number + 1\n",
    "        bg_id = bg_id + len(word_token) - 3\n",
    "\n",
    "        visualize_ner_tags(example['tokens'], ner_tags)\n",
    "\n",
    "    ner_tags[0] = -100\n",
    "    ner_tags[-1] = -100\n",
    "    return ner_tags\n",
    "\n",
    "def assign_ner_tags_roberta(example):\n",
    "    # print(example['sentence'])\n",
    "    token_input = tokenizer(example['sentence'])\n",
    "    example['tokens'] = tokenizer.convert_ids_to_tokens(\n",
    "        token_input['input_ids'])\n",
    "\n",
    "    # print(example['sentence'])\n",
    "    # print(example['tokens'])\n",
    "    # print(\"len: \", len(example['tokens']))\n",
    "\n",
    "    ner_tags = [0 for token in example['tokens']]\n",
    "    if str(type(example['locs'])) == \"<class 'list'>\":\n",
    "        locs = example['locs']\n",
    "    else:\n",
    "        locs = ast.literal_eval(example['locs'])\n",
    "\n",
    "    locs = [(int(loc[0]), int(loc[1]), loc[2]) for loc in locs]\n",
    "    locs = sorted(locs)\n",
    "    # print(locs)\n",
    "    bg_id = 1\n",
    "    pre_loc = 0\n",
    "    text = example['sentence']\n",
    "    for loc in locs:\n",
    "        loc0 = int(loc[0])\n",
    "        loc1 = int(loc[1])\n",
    "\n",
    "        pre_text = text[pre_loc:loc0]\n",
    "        token_input = tokenizer(pre_text)\n",
    "        pre_token = tokenizer.convert_ids_to_tokens(\n",
    "            token_input['input_ids'])\n",
    "\n",
    "        # print(pre_text)\n",
    "        # print(pre_token)\n",
    "        # print(\"len: \", len(pre_token))\n",
    "\n",
    "        bg_id = bg_id + len(pre_token) - 2\n",
    "        pre_loc = loc1\n",
    "\n",
    "        # print(\"bg_id: \", bg_id)\n",
    "\n",
    "        word = example['sentence'][loc0: loc1]\n",
    "        token_input = tokenizer(word)\n",
    "        word_token = tokenizer.convert_ids_to_tokens(token_input['input_ids'])\n",
    "\n",
    "        # print(\"Word: \", word)\n",
    "        # print(word_token)\n",
    "        # print(\"len: \", len(word_token))\n",
    "\n",
    "        label_number = ner_tags_2_number[f\"B-{loc[2]}\"]\n",
    "        ner_tags[bg_id] = label_number\n",
    "        bg_id += 1\n",
    "        for idx in range(bg_id, bg_id + len(word_token) - 3):\n",
    "            ner_tags[idx] = label_number + 1\n",
    "        bg_id = bg_id + len(word_token) - 3\n",
    "\n",
    "        # visualize_ner_tags(example['tokens'], ner_tags)\n",
    "\n",
    "    ner_tags[0] = -100\n",
    "    ner_tags[-1] = -100\n",
    "    return ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëç „Äê folding & portable rebounder „Äë the jump trampoline is easily folded twice to be 1 / 4 size . it saves much storage space and easy to carry , you can store conveniently under the bed , in the closet , or in car trunk . portable for outdoor travel with your family , enjoy the entertainment from the mini trampoline .\n",
      "{'input_ids': [1, 507, 117545, 41131, 10759, 429, 5480, 97083, 507, 34243, 262, 3587, 31227, 269, 1166, 12324, 2736, 264, 282, 376, 840, 453, 884, 323, 278, 8620, 400, 1634, 754, 263, 639, 264, 1886, 366, 274, 295, 1106, 10928, 494, 262, 1518, 366, 267, 262, 6928, 366, 289, 267, 640, 10642, 323, 5480, 270, 2547, 1248, 275, 290, 495, 366, 929, 262, 3280, 292, 262, 3552, 31227, 323, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['[CLS]', '‚ñÅ', 'üëç', '‚ñÅ„Äê', '‚ñÅfolding', '‚ñÅ&', '‚ñÅportable', '‚ñÅrebounder', '‚ñÅ', '„Äë', '‚ñÅthe', '‚ñÅjump', '‚ñÅtrampoline', '‚ñÅis', '‚ñÅeasily', '‚ñÅfolded', '‚ñÅtwice', '‚ñÅto', '‚ñÅbe', '‚ñÅ1', '‚ñÅ/', '‚ñÅ4', '‚ñÅsize', '‚ñÅ.', '‚ñÅit', '‚ñÅsaves', '‚ñÅmuch', '‚ñÅstorage', '‚ñÅspace', '‚ñÅand', '‚ñÅeasy', '‚ñÅto', '‚ñÅcarry', '‚ñÅ,', '‚ñÅyou', '‚ñÅcan', '‚ñÅstore', '‚ñÅconveniently', '‚ñÅunder', '‚ñÅthe', '‚ñÅbed', '‚ñÅ,', '‚ñÅin', '‚ñÅthe', '‚ñÅcloset', '‚ñÅ,', '‚ñÅor', '‚ñÅin', '‚ñÅcar', '‚ñÅtrunk', '‚ñÅ.', '‚ñÅportable', '‚ñÅfor', '‚ñÅoutdoor', '‚ñÅtravel', '‚ñÅwith', '‚ñÅyour', '‚ñÅfamily', '‚ñÅ,', '‚ñÅenjoy', '‚ñÅthe', '‚ñÅentertainment', '‚ñÅfrom', '‚ñÅthe', '‚ñÅmini', '‚ñÅtrampoline', '‚ñÅ.', '[SEP]']\n",
      "[CLS] ‚ñÅ üëç ‚ñÅ„Äê ‚ñÅfolding ‚ñÅ& ‚ñÅportable ‚ñÅrebounder ‚ñÅ „Äë ‚ñÅthe ‚ñÅjump ‚ñÅtrampoline ‚ñÅis ‚ñÅeasily ‚ñÅfolded ‚ñÅtwice ‚ñÅto ‚ñÅbe ‚ñÅ1 ‚ñÅ/ ‚ñÅ4 ‚ñÅsize ‚ñÅ. ‚ñÅit ‚ñÅsaves ‚ñÅmuch ‚ñÅstorage ‚ñÅspace ‚ñÅand ‚ñÅeasy ‚ñÅto ‚ñÅcarry ‚ñÅ, ‚ñÅyou ‚ñÅcan ‚ñÅstore ‚ñÅconveniently ‚ñÅunder ‚ñÅthe ‚ñÅbed ‚ñÅ, ‚ñÅin ‚ñÅthe ‚ñÅcloset ‚ñÅ, ‚ñÅor ‚ñÅin ‚ñÅcar ‚ñÅtrunk ‚ñÅ. ‚ñÅportable ‚ñÅfor ‚ñÅoutdoor ‚ñÅtravel ‚ñÅwith ‚ñÅyour ‚ñÅfamily ‚ñÅ, ‚ñÅenjoy ‚ñÅthe ‚ñÅentertainment ‚ñÅfrom ‚ñÅthe ‚ñÅmini ‚ñÅtrampoline ‚ñÅ. [SEP] \n",
      "-100  0 0 0  11       0  11        12         0 0 0    0     0           0   0       0       0      0   0   0  0  0  0     0  0   0      0     0        0      0    0     0   0      0  0    0    0      0             0      0    0    0  0   0    0       0  0   0   0    0      0  11        0    0        0       0     0     0       0  0      0    0              0     0    0     0           0  -100  \n"
     ]
    }
   ],
   "source": [
    "id = 0\n",
    "print(raw_datasets['train'][id]['sentence'])\n",
    "token_input = tokenizer(raw_datasets['train'][id]['sentence'])\n",
    "print(token_input)\n",
    "words = tokenizer.convert_ids_to_tokens(token_input['input_ids'])\n",
    "print(words)\n",
    "labels = assign_ner_tags_roberta(raw_datasets['train'][id])\n",
    "\n",
    "visualize_ner_tags(words, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"sentence\"], truncation=True, is_split_into_words=False\n",
    "    )\n",
    "    tokenized_inputs[\"labels\"] = assign_ner_tags_bert(examples)\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2061\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2061\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2061\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "list_input_ids = []\n",
    "list_token_type_ids = []\n",
    "list_attention_mask = []\n",
    "list_labels = []\n",
    "\n",
    "for index, row in raw_dataset.iterrows():\n",
    "    try:\n",
    "        label = assign_ner_tags_bert(row)\n",
    "        token_input = tokenizer(row['sentence'])\n",
    "        list_input_ids.append(token_input['input_ids'])\n",
    "        # list_token_type_ids.append(token_input['token_type_ids'])\n",
    "        list_attention_mask.append(token_input['attention_mask'])\n",
    "        list_labels.append(label)\n",
    "    except Exception as error:\n",
    "        # print(error)\n",
    "        print(index)\n",
    "        print(row['sentence'])\n",
    "\n",
    "tokenized_datasets = pd.DataFrame()\n",
    "tokenized_datasets['input_ids'] = pd.Series(list_input_ids)\n",
    "# tokenized_datasets['token_type_ids'] = pd.Series(list_token_type_ids)\n",
    "tokenized_datasets['attention_mask'] = pd.Series(list_attention_mask)\n",
    "tokenized_datasets['labels'] = pd.Series(list_labels)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(tokenized_datasets)\n",
    "val_dataset = Dataset.from_pandas(tokenized_datasets)\n",
    "test_dataset = Dataset.from_pandas(tokenized_datasets)\n",
    "\n",
    "tokenized_datasets = DatasetDict(\n",
    "    {\"train\": train_dataset, \"val\": val_dataset, \"test\": test_dataset})\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1648\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 413\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets['train'].train_test_split(test_size=0.2, shuffle=True, seed=7)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-22 15:54:42.969834: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-22 15:54:42.991889: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-22 15:54:43.371264: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "label_names = processed_ner_tags\n",
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForTokenClassification: ['lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.classifier.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "# Modify code in heare\n",
    "from transformers.models.bert import modeling_bert\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "model.config.num_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Traning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=4,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"test\"], collate_fn=data_collator, batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.detach().cpu().clone().numpy()\n",
    "    labels = labels.detach().cpu().clone().numpy()\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    return true_labels, true_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 30\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac4f63852074404c815f7820d9942018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12360 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "output_dir = f\"./deberta-v3-large_v4_{num_train_epochs}e_split\"\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "best_f1_score = 0\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        # Necessary to pad predictions and labels for being gathered\n",
    "        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "        predictions_gathered = accelerator.gather(predictions)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "        metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    results = metric.compute()\n",
    "    print(\n",
    "        f\"epoch {epoch}:\",\n",
    "        {\n",
    "            key: results[f\"overall_{key}\"]\n",
    "            for key in [\"precision\", \"recall\", \"f1\", \"accuracy\"]\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Save and upload\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process and results[f\"overall_f1\"] > best_f1_score:\n",
    "        best_f1_score = results[f\"overall_f1\"]\n",
    "        print(f\"Save model at epoch {epoch} with better f1 score {best_f1_score}\")\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accelerator.wait_for_everyone()\n",
    "# unwrapped_model = accelerator.unwrap_model(model)\n",
    "# unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built-in traning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "model_name = 'bert-base-NER_v4_30e_split'\n",
    "args = TrainingArguments(\n",
    "    output_dir=f\"./{model_name}\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    overwrite_output_dir = 'True',\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=30,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanluuuuuuu/miniconda3/envs/one_for_all/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dd5074e74ea4275b0c7185a4c7ba0f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3030 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13467a850e14401af69443cb91d87d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanluuuuuuu/miniconda3/envs/one_for_all/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5390483140945435, 'eval_precision': 0.29554140127388534, 'eval_recall': 0.32357043235704325, 'eval_f1': 0.30892143808255657, 'eval_accuracy': 0.84631918323482, 'eval_runtime': 0.735, 'eval_samples_per_second': 276.184, 'eval_steps_per_second': 35.373, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3eaa157c24f481eaea43c3ed25b74b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4128625690937042, 'eval_precision': 0.4194312796208531, 'eval_recall': 0.49372384937238495, 'eval_f1': 0.4535554131966688, 'eval_accuracy': 0.8784524449220849, 'eval_runtime': 0.7446, 'eval_samples_per_second': 272.634, 'eval_steps_per_second': 34.919, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ced0cdf8a524be2b84ed52c7655d41f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3928360939025879, 'eval_precision': 0.4438138479001135, 'eval_recall': 0.5453277545327755, 'eval_f1': 0.48936170212765967, 'eval_accuracy': 0.8801719505642128, 'eval_runtime': 0.7553, 'eval_samples_per_second': 268.779, 'eval_steps_per_second': 34.425, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5fda39692fe4bd486f0dd266e2f3ad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.39923277497291565, 'eval_precision': 0.49937733499377335, 'eval_recall': 0.5592747559274756, 'eval_f1': 0.5276315789473683, 'eval_accuracy': 0.890918860827512, 'eval_runtime': 0.7468, 'eval_samples_per_second': 271.841, 'eval_steps_per_second': 34.817, 'epoch': 4.0}\n",
      "{'loss': 0.4004, 'learning_rate': 1.66996699669967e-05, 'epoch': 4.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81134c16c4e544ecb43ffae1778b74ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.41262272000312805, 'eval_precision': 0.4616216216216216, 'eval_recall': 0.595536959553696, 'eval_f1': 0.5200974421437272, 'eval_accuracy': 0.8870499731327244, 'eval_runtime': 0.751, 'eval_samples_per_second': 270.292, 'eval_steps_per_second': 34.619, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca3d5a2e34d144d7b494b8c28e743eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.47546905279159546, 'eval_precision': 0.49019607843137253, 'eval_recall': 0.592747559274756, 'eval_f1': 0.5366161616161615, 'eval_accuracy': 0.892315959161741, 'eval_runtime': 0.7423, 'eval_samples_per_second': 273.456, 'eval_steps_per_second': 35.024, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a71c54073e5f4af2adc18164cffab46a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4987986087799072, 'eval_precision': 0.48931383577052867, 'eval_recall': 0.606694560669456, 'eval_f1': 0.5417185554171856, 'eval_accuracy': 0.8878022568511553, 'eval_runtime': 0.7411, 'eval_samples_per_second': 273.926, 'eval_steps_per_second': 35.084, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c6ca8cbaeec40e38330507923b80630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5070109367370605, 'eval_precision': 0.4874715261958998, 'eval_recall': 0.596931659693166, 'eval_f1': 0.5366771159874609, 'eval_accuracy': 0.891456206340677, 'eval_runtime': 0.7482, 'eval_samples_per_second': 271.328, 'eval_steps_per_second': 34.751, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dee153343cd49558b914a294c2daba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5372799038887024, 'eval_precision': 0.46912242686890576, 'eval_recall': 0.603905160390516, 'eval_f1': 0.5280487804878049, 'eval_accuracy': 0.8886620096722192, 'eval_runtime': 0.7413, 'eval_samples_per_second': 273.826, 'eval_steps_per_second': 35.071, 'epoch': 9.0}\n",
      "{'loss': 0.066, 'learning_rate': 1.33993399339934e-05, 'epoch': 9.9}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fd55dfa386a46e4b72567f79e1ca311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5576326847076416, 'eval_precision': 0.505656108597285, 'eval_recall': 0.6234309623430963, 'eval_f1': 0.5584009993753903, 'eval_accuracy': 0.893175711982805, 'eval_runtime': 0.7493, 'eval_samples_per_second': 270.908, 'eval_steps_per_second': 34.698, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d8407d380ae45d2a0e3e831c1e2d4f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5933877229690552, 'eval_precision': 0.5267441860465116, 'eval_recall': 0.6317991631799164, 'eval_f1': 0.5745085605580215, 'eval_accuracy': 0.8927458355722729, 'eval_runtime': 0.7405, 'eval_samples_per_second': 274.127, 'eval_steps_per_second': 35.11, 'epoch': 11.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee2df1ad9619490a8bbdc0e839fd8219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5909303426742554, 'eval_precision': 0.48467650397275824, 'eval_recall': 0.595536959553696, 'eval_f1': 0.5344180225281602, 'eval_accuracy': 0.8894142933906501, 'eval_runtime': 0.7424, 'eval_samples_per_second': 273.452, 'eval_steps_per_second': 35.023, 'epoch': 12.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25245b57a9094ca78ffb4c07ddcc905c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5990766882896423, 'eval_precision': 0.5040091638029782, 'eval_recall': 0.6136680613668062, 'eval_f1': 0.5534591194968553, 'eval_accuracy': 0.8946802794196669, 'eval_runtime': 0.7492, 'eval_samples_per_second': 270.962, 'eval_steps_per_second': 34.705, 'epoch': 13.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7457fee576a40f7aeafa94d58989637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6326768398284912, 'eval_precision': 0.5082938388625592, 'eval_recall': 0.5983263598326359, 'eval_f1': 0.549647661755285, 'eval_accuracy': 0.892101020956475, 'eval_runtime': 0.7451, 'eval_samples_per_second': 272.442, 'eval_steps_per_second': 34.894, 'epoch': 14.0}\n",
      "{'loss': 0.0234, 'learning_rate': 1.00990099009901e-05, 'epoch': 14.85}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969c27fc84164375a9c2ae87f3da1a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6144258379936218, 'eval_precision': 0.5234093637454982, 'eval_recall': 0.6080892608089261, 'eval_f1': 0.5625806451612904, 'eval_accuracy': 0.8942504030091348, 'eval_runtime': 0.7497, 'eval_samples_per_second': 270.791, 'eval_steps_per_second': 34.683, 'epoch': 15.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f631c4defa5947d6a7141021b4be09a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6500821709632874, 'eval_precision': 0.5101311084624554, 'eval_recall': 0.596931659693166, 'eval_f1': 0.5501285347043702, 'eval_accuracy': 0.89156367544331, 'eval_runtime': 0.7536, 'eval_samples_per_second': 269.372, 'eval_steps_per_second': 34.501, 'epoch': 16.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c4f270236cc4f6eb97ea08a87ebe4ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6861269474029541, 'eval_precision': 0.5338983050847458, 'eval_recall': 0.6150627615062761, 'eval_f1': 0.5716137394685677, 'eval_accuracy': 0.893068242880172, 'eval_runtime': 0.7466, 'eval_samples_per_second': 271.901, 'eval_steps_per_second': 34.825, 'epoch': 17.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a681107992474389beee70383153585e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6850368976593018, 'eval_precision': 0.5, 'eval_recall': 0.6178521617852162, 'eval_f1': 0.5527136618839675, 'eval_accuracy': 0.8929607737775389, 'eval_runtime': 0.7527, 'eval_samples_per_second': 269.683, 'eval_steps_per_second': 34.541, 'epoch': 18.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8581555fcd504071a138d735a1dd6a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6882691979408264, 'eval_precision': 0.506993006993007, 'eval_recall': 0.606694560669456, 'eval_f1': 0.5523809523809524, 'eval_accuracy': 0.8927458355722729, 'eval_runtime': 0.747, 'eval_samples_per_second': 271.77, 'eval_steps_per_second': 34.808, 'epoch': 19.0}\n",
      "{'loss': 0.0111, 'learning_rate': 6.798679867986799e-06, 'epoch': 19.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be7bb69858bb405fa4d784466b63db74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7034802436828613, 'eval_precision': 0.49272116461366183, 'eval_recall': 0.6136680613668062, 'eval_f1': 0.546583850931677, 'eval_accuracy': 0.8937130574959699, 'eval_runtime': 0.7523, 'eval_samples_per_second': 269.832, 'eval_steps_per_second': 34.56, 'epoch': 20.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26418f1d00964a349e61904584d4f87c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7085552215576172, 'eval_precision': 0.5056947608200456, 'eval_recall': 0.6192468619246861, 'eval_f1': 0.5567398119122258, 'eval_accuracy': 0.892101020956475, 'eval_runtime': 0.7477, 'eval_samples_per_second': 271.508, 'eval_steps_per_second': 34.774, 'epoch': 21.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e473cde0f5640a8a014d023d5e95d19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7159950137138367, 'eval_precision': 0.49774774774774777, 'eval_recall': 0.6164574616457462, 'eval_f1': 0.550778816199377, 'eval_accuracy': 0.89263836646964, 'eval_runtime': 0.7421, 'eval_samples_per_second': 273.536, 'eval_steps_per_second': 35.034, 'epoch': 22.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0c1c0a0edf0453b943ca18edffd352f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.716690719127655, 'eval_precision': 0.5022779043280182, 'eval_recall': 0.6150627615062761, 'eval_f1': 0.5529780564263322, 'eval_accuracy': 0.8940354648038689, 'eval_runtime': 0.7519, 'eval_samples_per_second': 269.98, 'eval_steps_per_second': 34.579, 'epoch': 23.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce2e5bca1644f41a8a0c69704962bf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.732965350151062, 'eval_precision': 0.5022779043280182, 'eval_recall': 0.6150627615062761, 'eval_f1': 0.5529780564263322, 'eval_accuracy': 0.893175711982805, 'eval_runtime': 0.7431, 'eval_samples_per_second': 273.186, 'eval_steps_per_second': 34.989, 'epoch': 24.0}\n",
      "{'loss': 0.0056, 'learning_rate': 3.4983498349834986e-06, 'epoch': 24.75}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db18ca118dc24b3f8d237ededfff261f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7256616950035095, 'eval_precision': 0.5126436781609195, 'eval_recall': 0.6220362622036262, 'eval_f1': 0.5620667926906112, 'eval_accuracy': 0.8960773777538957, 'eval_runtime': 0.7401, 'eval_samples_per_second': 274.28, 'eval_steps_per_second': 35.129, 'epoch': 25.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc48122002e4d4bbe9e7d6eeafc8a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7405313849449158, 'eval_precision': 0.49776785714285715, 'eval_recall': 0.6220362622036262, 'eval_f1': 0.5530068195908245, 'eval_accuracy': 0.8918860827512091, 'eval_runtime': 0.7519, 'eval_samples_per_second': 269.969, 'eval_steps_per_second': 34.577, 'epoch': 26.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "731305a1b2a945a18f72d8dfa871f32e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7301244735717773, 'eval_precision': 0.511441647597254, 'eval_recall': 0.6234309623430963, 'eval_f1': 0.5619107479572596, 'eval_accuracy': 0.8955400322407308, 'eval_runtime': 0.7496, 'eval_samples_per_second': 270.808, 'eval_steps_per_second': 34.685, 'epoch': 27.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6994988f4f444189d14b5a63d15f4a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7357547283172607, 'eval_precision': 0.5198598130841121, 'eval_recall': 0.6206415620641562, 'eval_f1': 0.5657978385251112, 'eval_accuracy': 0.8951101558301988, 'eval_runtime': 0.7494, 'eval_samples_per_second': 270.868, 'eval_steps_per_second': 34.693, 'epoch': 28.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d5d16612ab8442ebe22d668d1a33c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7418428063392639, 'eval_precision': 0.5103211009174312, 'eval_recall': 0.6206415620641562, 'eval_f1': 0.5601006922592826, 'eval_accuracy': 0.8938205265986029, 'eval_runtime': 0.7499, 'eval_samples_per_second': 270.693, 'eval_steps_per_second': 34.67, 'epoch': 29.0}\n",
      "{'loss': 0.0031, 'learning_rate': 1.9801980198019803e-07, 'epoch': 29.7}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6faad3b1b44d40e08bc65c0b1f39cbf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7424449920654297, 'eval_precision': 0.5097365406643757, 'eval_recall': 0.6206415620641562, 'eval_f1': 0.5597484276729561, 'eval_accuracy': 0.8939279957012359, 'eval_runtime': 0.7455, 'eval_samples_per_second': 272.294, 'eval_steps_per_second': 34.875, 'epoch': 30.0}\n",
      "{'train_runtime': 360.4122, 'train_samples_per_second': 67.256, 'train_steps_per_second': 8.407, 'train_loss': 0.08410851382112543, 'epoch': 30.0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model(f\"./{model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-23 20:05:53.551834: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-23 20:05:53.576448: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-23 20:05:53.960097: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from collections import defaultdict\n",
    "\n",
    "# Replace this with your own checkpoint\n",
    "model_checkpoint = \"./deberta-v3-large_v4_30e_split\"\n",
    "token_classifier = pipeline(\n",
    "    \"ner\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ergonomic papasan chair for everyone : the heavy weight capacity frame is 40 inches wide ( included papasan chair cushion ) and 20 inches deep , making our rattan reading chair suitable and comfortable for different types of bodies ; the papasan chair with cushion and frame can support up to 400 pounds\n"
     ]
    }
   ],
   "source": [
    "def preprocess_description(description):\n",
    "    single_description = description.lower().strip()\n",
    "\n",
    "    # This description contains space between the first and second letter.\n",
    "    # special_description = '''\n",
    "    # ‚òÄÔ∏è„Äêlarge size„Äë- the size of the chair is w23.2 \"* l21\" * h40 \", the sitting height is 17\", and they also have a 40 \"high back and a 29\" sitting depth. compared with most chairs on the market, we have increased the back, widened the arm spacing, and lengthened the depth of the seat. you have a stronger sense of space when sitting, and more space for activities.\n",
    "    # '''.strip()\n",
    "\n",
    "    new_description = []\n",
    "    last_special = -1\n",
    "    for idx, letter in enumerate(single_description):\n",
    "        if not (('a' <= letter and letter <= 'z') or ('0' <= letter and letter <= '9') or letter == ' '):\n",
    "            pretext = single_description[last_special + 1:idx].strip()\n",
    "            if pretext != '' and pretext != ' ':\n",
    "                new_description.append(pretext)\n",
    "            new_description.append(letter.strip())\n",
    "            last_special = idx\n",
    "        if idx == len(single_description) - 1:\n",
    "            new_description.append(single_description[last_special + 1:idx + 1].strip())\n",
    "    return \" \".join(new_description)\n",
    "print(preprocess_description(\"ERGONOMIC PAPASAN CHAIR FOR EVERYONE: The heavy weight capacity frame is 40 inches wide (included papasan chair cushion) and 20 inches deep, making our rattan reading chair suitable and comfortable for different types of bodies; The papasan chair with cushion and frame can support up to 400 pounds\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 % metal , 20 % rattan , 70 % polyester\n",
      "--- HIGH SCORE ---\n",
      "MAT: {'20 % rattan', '10 % metal', '70 % polyester'}\n",
      "--- LOW SCORE ---\n",
      "\n",
      "space efficiency : with a linen accent sofa chair sized 27 \" d x 25 \" w x 34 \" h . this upholstered accent sofa chair with rattan armrest takes up limited space for living room , bedroom , game room , lounge or office . \n",
      "--- HIGH SCORE ---\n",
      "MAT: {'linen'}\n",
      "DIMENSION: {'27 \" d x 25 \" w x 34 \" h'}\n",
      "PROPERTY: {'rattan armrest', 'upholstered'}\n",
      "--- LOW SCORE ---\n",
      "\n",
      "swivel function : you can sit on the swivel sofa chair for gatherings or family time . you can turn it to face different directions easily . with its 360 ¬∞ rotation capability , you ' ll effortlessly find the perfect position for ultimate relaxation and comfort . \n",
      "--- HIGH SCORE ---\n",
      "PROPERTY: {'360 ¬∞ rotation capability', 'swivel'}\n",
      "--- LOW SCORE ---\n",
      "\n",
      "roomy & cozy : our linen accent sofa chair is crafted for your ultimate relaxation . the upholstered accent chair with rattan armrest is designed to provide support for your arms . whether you ' re settling in for an evening of reading , gaming , or enjoying a movie , this single sofa chair is your ideal companion for unwinding and making the most of your leisure time . \n",
      "--- HIGH SCORE ---\n",
      "MAT: {'linen'}\n",
      "PROPERTY: {'rattan armrest', 'upholstered'}\n",
      "--- LOW SCORE ---\n",
      "\n",
      "robust base & frame : the 360 - degree rotating base is made of durable metal material . rest easy knowing our accent sofa chair boasts a solid frame and high - quality springs . you can use this swivel accent chair with rattan handrails for a long time . \n",
      "--- HIGH SCORE ---\n",
      "PROPERTY: {'solid frame', 'robust base & frame', 'rattan handrails', '360 - degree rotating base', 'springs', 'swivel'}\n",
      "MAT: {'metal'}\n",
      "--- LOW SCORE ---\n",
      "\n",
      "simple assembly : each package comes with a clear and detailed assembly instruction guide . putting together this armchair is a easy task . get ready to welcome your room ' s new , favorite furniture companion with this chair . \n",
      "--- HIGH SCORE ---\n",
      "--- LOW SCORE ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "description = '''\n",
    "10% Metal, 20% Rattan, 70% Polyester\n",
    "Space Efficiency: With a linen accent sofa chair sized 27\"D x 25\"W x 34\"H. This upholstered accent sofa chair with rattan armrest takes up limited space for living room, bedroom, game room, lounge or office.\n",
    "Swivel Function: You can sit on the swivel sofa chair for gatherings or family time. You can turn it to face different directions easily. With its 360¬∞rotation capability, you'll effortlessly find the perfect position for ultimate relaxation and comfort.\n",
    "Roomy & Cozy: Our linen accent sofa chair is crafted for your ultimate relaxation. The upholstered accent chair with rattan armrest is designed to provide support for your arms. Whether you're settling in for an evening of reading, gaming, or enjoying a movie, this single sofa chair is your ideal companion for unwinding and making the most of your leisure time.\n",
    "Robust Base & Frame: The 360-degree rotating base is made of durable metal material. Rest easy knowing our accent sofa chair boasts a solid frame and high-quality springs. You can use this swivel accent chair with rattan handrails for a long time.\n",
    "Simple Assembly: Each package comes with a clear and detailed assembly instruction guide. Putting together this armchair is a easy task. Get ready to welcome your room's new, favorite furniture companion with this chair.\n",
    "'''\n",
    "\n",
    "bullet_points = description.split(\"\\n\")\n",
    "for bullet_point in bullet_points:\n",
    "    bullet_point = preprocess_description(bullet_point)\n",
    "\n",
    "    if bullet_point != \"\":\n",
    "        print(bullet_point)\n",
    "\n",
    "        low_score_ans = defaultdict(set)\n",
    "        high_score_ans = defaultdict(set)\n",
    "        results = token_classifier(bullet_point)\n",
    "        for res in results:\n",
    "            group = res['entity_group']\n",
    "            if res['score'] >= 0.5:\n",
    "                high_score_ans[group].add(res['word'].lower())\n",
    "            else:\n",
    "                low_score_ans[group].add(res['word'].lower())\n",
    "\n",
    "        print(\"--- HIGH SCORE ---\")\n",
    "        for key in high_score_ans.keys():\n",
    "            line = f\"{key}: {high_score_ans[key]}\"\n",
    "            print(line)\n",
    "\n",
    "        print(\"--- LOW SCORE ---\")\n",
    "        for key in low_score_ans.keys():\n",
    "            line = f\"{key}: {low_score_ans[key]}\"\n",
    "            print(line)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- HIGH SCORE ---\n",
      "PROPERTY: {'airy tc inner fabric', '360 degree swivel', 'adjustable', 'smoothly rotating structure', 'thick cushion', 'five levelers', 'padding', 'wicker', 'cushion and', 'three straps', '360 degree swivel papasan chair base', 'papasan cushion', 'elegant design', 'heavy weight capacity frame', 'frame', 'round wicker chair frame', 'papasan chair cushion'}\n",
      "DIMENSION: {'40 inches wide', '6 3/4 inches thickness', '20 inches deep'}\n",
      "MAT: {'solid metal', 'fiber', 'rattan', 'woven resin wicker'}\n",
      "WEIGHT: {'9.48 pounds', '400 pounds'}\n",
      "SHAPE: {'rounded shape'}\n",
      "--- LOW SCORE ---\n"
     ]
    }
   ],
   "source": [
    "description = '''\n",
    "ERGONOMIC PAPASAN CHAIR FOR EVERYONE: The heavy weight capacity frame is 40 inches wide (included papasan chair cushion) and 20 inches deep, making our rattan reading chair suitable and comfortable for different types of bodies; The papasan chair with cushion and frame can support up to 400 pounds\n",
    "\n",
    "HEAVENLY COMFORT WITH PLUMP CUSHION: Our upgraded papasan chair cushion has 9.48 pounds of extra padding fiber, 6 3/4 inches thickness, plus an airy TC inner fabric for added comfort and coziness; Three straps secure the large papasan cushion on the round wicker chair frame, preventing it from slipping away\n",
    "\n",
    "360 DEGREE SWIVEL PAPASAN CHAIR: The papasan chair frame is made of solid metal and expertly woven resin wicker; 360 degree swivel papasan chair base has a smoothly rotating structure without noises and friction; Five levelers protect your floor and are easily adjustable for stability of wicker papasan chair\n",
    "\n",
    "VERSATILE WICKER PAPASAN CHAIR: Our large papasan chair, with its rounded shape and extra thick cushion, adds a cozy inviting space to relax whether you're sitting or curling up; The chair's elegant design complements any room decor, the suitable addition to your living room, bedroom, library or leisure lounge\n",
    "QUICK AND SIMPLE TO ASSEMBLE: Follow our manual to quickly install your reading chair; All necessary accessories, as well as extra backup screws and levelers, are included; With a dedicated customer service team to assist you with any problems; When you acquire our papasan chair with cushion and frame, do not worry about after sales service\n",
    "'''\n",
    "\n",
    "low_score_ans = defaultdict(set)\n",
    "high_score_ans = defaultdict(set)\n",
    "\n",
    "bullet_points = description.split(\"\\n\")\n",
    "for bullet_point in bullet_points:\n",
    "    bullet_point = bullet_point.strip()\n",
    "    if bullet_point != \"\":\n",
    "        results = token_classifier(bullet_point)\n",
    "        for res in results:\n",
    "            group = res['entity_group']\n",
    "            if res['score'] >= 0.5:\n",
    "                high_score_ans[group].add(res['word'].lower())\n",
    "            else:\n",
    "                low_score_ans[group].add(res['word'].lower())\n",
    "\n",
    "print(\"--- HIGH SCORE ---\")\n",
    "for key in high_score_ans.keys():\n",
    "    line = f\"{key}: {high_score_ans[key]}\"\n",
    "    print(line)\n",
    "\n",
    "print(\"--- LOW SCORE ---\")\n",
    "for key in low_score_ans.keys():\n",
    "    line = f\"{key}: {low_score_ans[key]}\"\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PROPERTY',\n",
       "  'score': 0.9972402,\n",
       "  'word': 'inner - weave',\n",
       "  'start': 14,\n",
       "  'end': 25},\n",
       " {'entity_group': 'PROPERTY',\n",
       "  'score': 0.99640507,\n",
       "  'word': 'cross - stitched canvas inside lining',\n",
       "  'start': 27,\n",
       "  'end': 62}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Replace this with your own checkpoint\n",
    "model_checkpoint = \"./bert-finetuned-ner\"\n",
    "model = pipeline(\n",
    "    \"ner\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    ")\n",
    "token_classifier(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-08 14:47:20.625248: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-08 14:47:20.650816: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-08 14:47:21.018300: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new movie is awesome \t\t The new movie is so great \t\t Score: 0.8939\n",
      "The cat sits outside \t\t The cat plays in the garden \t\t Score: 0.6788\n",
      "I love pasta \t\t Do you like pizza? \t\t Score: 0.5096\n",
      "I love pasta \t\t The new movie is so great \t\t Score: 0.2560\n",
      "I love pasta \t\t The new movie is awesome \t\t Score: 0.2440\n",
      "A man is playing guitar \t\t The cat plays in the garden \t\t Score: 0.2105\n",
      "The new movie is awesome \t\t Do you like pizza? \t\t Score: 0.1969\n",
      "The new movie is so great \t\t Do you like pizza? \t\t Score: 0.1692\n",
      "The cat sits outside \t\t A woman watches TV \t\t Score: 0.1310\n",
      "The cat plays in the garden \t\t Do you like pizza? \t\t Score: 0.0900\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Single list of sentences - Possible tens of thousands of sentences\n",
    "sentences = ['The cat sits outside',\n",
    "             'A man is playing guitar',\n",
    "             'I love pasta',\n",
    "             'The new movie is awesome',\n",
    "             'The cat plays in the garden',\n",
    "             'A woman watches TV',\n",
    "             'The new movie is so great',\n",
    "             'Do you like pizza?']\n",
    "\n",
    "paraphrases = util.paraphrase_mining(model, sentences)\n",
    "\n",
    "for paraphrase in paraphrases[0:10]:\n",
    "    score, i, j = paraphrase\n",
    "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences[i], sentences[j], score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/tanluuuuuuu/miniconda3/envs/one_for_all/lib/python3.8/site-packages/transformers/utils/import_utils.py\", line 1099, in _get_module\n",
      "  File \"/home/tanluuuuuuu/miniconda3/envs/one_for_all/lib/python3.8/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 973, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'transformers.models.bark.configuration_bark'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tanluuuuuuu/miniconda3/envs/one_for_all/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_1263886/2283165407.py\", line 5, in <module>\n",
      "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
      "  File \"/home/tanluuuuuuu/miniconda3/envs/one_for_all/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py\", line 95, in __init__\n",
      "    modules = self._load_sbert_model(model_path)\n",
      "  File \"/home/tanluuuuuuu/miniconda3/envs/one_for_all/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py\", line 840, in _load_sbert_model\n",
      "    module = module_class.load(os.path.join(model_path, module_config['path']))\n",
      "  File \"/home/tanluuuuuuu/miniconda3/envs/one_for_all/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py\", line 137, in load\n",
      "    return Transformer(model_name_or_path=input_path, **config)\n",
      "  File \"/home/tanluuuuuuu/miniconda3/envs/one_for_all/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py\", line 29, in __init__\n",
      "    self._load_model(model_name_or_path, config, cache_dir)\n",
      "  File \"/home/tanluuuuuuu/miniconda3/envs/one_for_all/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py\", line 49, in _load_model\n",
      "    self.auto_model = AutoModel.from_pretrained(model_name_or_path, config=config, cache_dir=cache_dir)\n",
      "  File \"/home/tanluuuuuuu/miniconda3/envs/one_for_all/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\", line 474, in from_pretrained\n",
      "    class_ref = config.auto_map[cls.__name__]\n",
      "  File \"/home/tanluuuuuuu/miniconda3/envs/one_for_all/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\", line 683, in keys\n",
      "    return self.__getitem__(key)\n",
      "  File \"/home/tanluuuuuuu/miniconda3/envs/one_for_all/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\", line 684, in <listcomp>\n",
      "    except KeyError:\n",
      "  File \"/home/tanluuuuuuu/miniconda3/envs/one_for_all/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\", line 680, in _load_attr_from_module\n",
      "  File \"/home/tanluuuuuuu/miniconda3/envs/one_for_all/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\", line 625, in getattribute_from_module\n",
      "    except ValueError:\n",
      "  File \"/home/tanluuuuuuu/miniconda3/envs/one_for_all/lib/python3.8/site-packages/transformers/utils/import_utils.py\", line 1089, in __getattr__\n",
      "    f\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\n",
      "  File \"/home/tanluuuuuuu/miniconda3/envs/one_for_all/lib/python3.8/site-packages/transformers/utils/import_utils.py\", line 1101, in _get_module\n",
      "    def direct_transformers_import(path: str, file=\"__init__.py\") -> ModuleType:\n",
      "RuntimeError: Failed to import transformers.models.bark.configuration_bark because of the following error (look up to see its traceback):\n",
      "No module named 'transformers.models.bark.configuration_bark'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tanluuuuuuu/miniconda3/envs/one_for_all/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2102, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/tanluuuuuuu/miniconda3/envs/one_for_all/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1310, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/tanluuuuuuu/miniconda3/envs/one_for_all/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1199, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/tanluuuuuuu/miniconda3/envs/one_for_all/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1052, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/tanluuuuuuu/miniconda3/envs/one_for_all/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 978, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/home/tanluuuuuuu/miniconda3/envs/one_for_all/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 878, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/home/tanluuuuuuu/miniconda3/envs/one_for_all/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 712, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/home/tanluuuuuuu/miniconda3/envs/one_for_all/lib/python3.8/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/tanluuuuuuu/miniconda3/envs/one_for_all/lib/python3.8/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/home/tanluuuuuuu/miniconda3/envs/one_for_all/lib/python3.8/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/tanluuuuuuu/miniconda3/envs/one_for_all/lib/python3.8/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/home/tanluuuuuuu/miniconda3/envs/one_for_all/lib/python3.8/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/tanluuuuuuu/miniconda3/envs/one_for_all/lib/python3.8/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"/home/tanluuuuuuu/miniconda3/envs/one_for_all/lib/python3.8/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from collections import defaultdict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "list_noun_phrase = [\n",
    "    'vinyl coated', 'wide, smooth handle', 'durable vinyl coated finish', 'textured handle', 'increase durability', 'stability', 'flat bottom', 'reduce noise', 'protect flooring', 'any age', 'wide range of weights', 'prevent corrosion', 'high-quality', 'comfortable & secure grip', 'upright storage',\n",
    "    'bands', 'resistance bands', 'exercise band',\n",
    "    'all-purpose color', 'easier storage', 'better grip', 'coated', 'easy recognition of the weight', 'prevent rolling',\n",
    "    'flat bottom', 'textured wide handle', 'non-slip grip',\n",
    "    'glossy finish wide', 'smooth handle', 'black paint finish', 'smooth, slightly textured handle',\n",
    "    'solid smooth', 'rubber base', 'wide, smooth handle', 'smooth, slightly textured handle',\n",
    "    'increase durability', 'functional fitness', 'versatile', 'stability', 'flat bottom', 'better grip strength', 'adjustable', 'prevent corrosion', 'glossy finish', 'comfortable & secure grip', 'upright storage',\n",
    "\n",
    "    'easier on wrist', 'precision made', 'stamped in', 'strong grip handles', 'forearm', 'easier to maintain a strong', 'durable', 'recessed logos', 'powder coating', 'smooth finish', 'gravity casting', '##y', 'close'\n",
    "]\n",
    "embeddings = model.encode(list_noun_phrase)\n",
    "\n",
    "\n",
    "cluster = DBSCAN(eps = 0.4, min_samples = 1, metric = 'cosine').fit(embeddings)\n",
    "\n",
    "\n",
    "clusters = defaultdict(list)\n",
    "\n",
    "for i, label in enumerate(cluster.labels_):\n",
    "    clusters[label].append(list_noun_phrase[i])\n",
    "display(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix format COLNN2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëâ -X- _ O\n",
      "„Äê -X- _ O\n",
      "EASY -X- _ B-FEATURE\n"
     ]
    }
   ],
   "source": [
    "label0 = '-X- _ O'\n",
    "\n",
    "line = 'üëâ„ÄêEASY -X- _ B-FEATURE'\n",
    "\n",
    "word = line.split(\" \")[0]\n",
    "label = line.split(\" \")[1:]\n",
    "\n",
    "idx = word.find(\"„Äê\")\n",
    "\n",
    "line1 = word[:idx] + \" \" + label0\n",
    "\n",
    "line2 = \"„Äê\" + \" \" + label0\n",
    "\n",
    "line3 = word[idx + 1:] + \" \" + \" \".join(label) \n",
    "\n",
    "print(line1)\n",
    "print(line2)\n",
    "print(line3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-X-', '_', 'B-FEATURE']\n",
      "„Äê -X- _ O\n",
      "UNFILLED -X- _ B-FEATURE\n",
      "„Äë -X- _ O\n"
     ]
    }
   ],
   "source": [
    "LABEL0 = '-X- _ O'\n",
    "\n",
    "line = '„ÄêUNFILLED„Äë -X- _ B-FEATURE'\n",
    "\n",
    "word = line.split(\" \")[0]\n",
    "label = line.split(\" \")[1:]\n",
    "print(label)\n",
    "\n",
    "idxl = word.find(\"„Äê\")\n",
    "idxr = word.find(\"„Äë\")\n",
    "\n",
    "line1 = word[:idxl + 1] + \" \" + LABEL0\n",
    "\n",
    "line2 = word[idxl + 1: idxr] + \" \" + \" \".join(label) \n",
    "\n",
    "line3 = word[idxr: ] + \" \" + LABEL0\n",
    "\n",
    "print(line1)\n",
    "print(line2)\n",
    "print(line3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessories -X- _ O\n",
      "„Äë -X- _ O\n",
      "IncludesÔºö1 -X- _ B-PROPERTY\n"
     ]
    }
   ],
   "source": [
    "LABEL0 = '-X- _ O'\n",
    "\n",
    "line = 'Accessories„ÄëIncludesÔºö1 -X- _ B-PROPERTY'\n",
    "line = 'Waterproof„ÄëThe -X- _ B-FEATURE'\n",
    "\n",
    "word = line.split(\" \")[0]\n",
    "label = line.split(\" \")[1:]\n",
    "\n",
    "idx = word.find(\"„Äë\")\n",
    "\n",
    "if \"I-\" in \" \".join(label):\n",
    "    line1 = word[:idx] + \" \" + \" \".join(label)\n",
    "else:\n",
    "    line1 = word[:idx] + \" \" + LABEL0\n",
    "\n",
    "line2 = \"„Äë\" + \" \" + LABEL0 \n",
    "\n",
    "if \"B-\" in \" \".join(label):\n",
    "    line3 = word[idx + 1: ] + \" \" + \" \".join(label)\n",
    "else:\n",
    "    line3 = word[idx + 1: ] + \" \" + LABEL0\n",
    "\n",
    "print(line1)\n",
    "print(line2)\n",
    "print(line3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
